# filepath: /Users/tillermold/Desktop/CODE/Synthesis Semester/Capstone_Project/cp-swiss-german-asr/configs/training/german_adaptation.yml
# German Adaptation Configuration with Elastic Weight Consolidation (EWC)
# Fine-tuning phase for Dutch-pretrained model on German Common Voice data
# Uses EWC to prevent catastrophic forgetting of Dutch knowledge

model:
  dutch_checkpoint: "models/pretrained/wav2vec2-dutch" # placeholder for Dutch pre-trained model path
  freeze_feature_encoder: false  # Allow fine-tuning of all layers

data:
  train_metadata: "data/metadata/german/train.tsv"
  val_metadata: "data/metadata/german/validation.tsv"
  sampling_rate: 16000
  max_duration_seconds: 20
  num_workers: 4

training:
  # Optimizer settings (lower learning rate for adaptation)
  learning_rate: 1e-4  # Lower than Dutch pre-training (3e-5)
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Training schedule (fewer epochs for adaptation)
  num_epochs: 3  # Fewer than Dutch pre-training (10)
  warmup_steps: 300  # Fewer warmup steps
  max_steps: -1
  
  # Batch settings
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2  # Effective batch size: 8 * 2 = 16
  
  # Mixed precision
  fp16: true
  fp16_opt_level: "O1"
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Regularization
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  feat_proj_dropout: 0.1
  layerdrop: 0.05  # Lower than pre-training for stability

# Elastic Weight Consolidation (EWC) settings
ewc:
  enabled: true
  lambda: 0.4  # Regularization strength (0=no EWC, higher=more preservation)
  fisher_samples: 1000  # Number of samples for Fisher matrix computation
  compute_fisher: true  # Whether to compute Fisher matrix

checkpointing:
  output_dir: "models/pretrained/wav2vec2-german-adapted"
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3  # Keep best 3 checkpoints
  load_best_model_at_end: true  # Load best model based on validation loss

logging:
  logging_strategy: "steps"
  logging_steps: 100
  report_to: ["tensorboard"]
  logging_dir: "results/logs/tensorboard/german_adaptation"

evaluation:
  evaluation_strategy: "steps"
  eval_steps: 500  # Evaluate every 500 steps
  metric_for_best_model: "loss"
  greater_is_better: false

early_stopping:
  enabled: true
  patience: 3  # Stop if no improvement for 3 evaluations

environment:
  seed: 42
  dataloader_num_workers: 4
  group_by_length: true
  length_column_name: "input_length"

# RunPod specific settings (applied when ENVIRONMENT=runpod)
runpod:
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  fp16: true
  dataloader_num_workers: 8
  eval_steps: 250  # More frequent evaluation on cloud GPU