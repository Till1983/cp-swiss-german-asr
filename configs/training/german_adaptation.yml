# German Adaptation Configuration with EWC - OPTIMIZED FOR RTX 5090 (32GB)
# This config uses larger batch sizes enabled by 32GB VRAM

model:
  dutch_checkpoint: "pretrained/wav2vec2-dutch-pretrained"
  freeze_feature_encoder: true
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  feat_proj_dropout: 0.1
  layerdrop: 0.05

data:
  train_metadata: "metadata/german/train.tsv"
  val_metadata: "metadata/german/val.tsv"
  sampling_rate: 16000
  max_duration_seconds: 20
  num_workers: 4

training:
  learning_rate: 1e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  num_train_epochs: 3
  warmup_steps: 300
  max_steps: -1
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  fp16: true
  fp16_opt_level: "O1"
  max_grad_norm: 1.0

ewc:
  enabled: true
  lambda: 0.4
  fisher_samples: 5000
  compute_fisher: true

checkpointing:
  output_dir: "adapted/wav2vec2-german-adapted"
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true

logging:
  logging_strategy: "steps"
  logging_steps: 100
  report_to: ["tensorboard"]
  logging_dir: "logs/tensorboard/german_adaptation"

evaluation:
  evaluation_strategy: "steps"
  eval_steps: 500
  metric_for_best_model: "loss"
  greater_is_better: false

early_stopping:
  enabled: true
  patience: 3

environment:
  seed: 42
  dataloader_num_workers: 4
  group_by_length: true
  length_column_name: "input_length"

# RTX 5090 specific settings - 32GB VRAM allows larger batches with EWC
runpod:
  per_device_train_batch_size: 8     # Doubled from 4 (32GB can handle it)
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2     # Halved from 4 (effective batch still 16)
  fp16: true
  dataloader_num_workers: 8
  eval_steps: 250